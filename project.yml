title: "Finnish model"
description: "Train spaCy model for Finnish on UD Finnish TDT"
spacy_version: ">=3.2.0,<4.0.0"
vars:
  vector_size: 50000
  vector_dim: 300
  max_texts: 1000000
  minn: 3
  maxn: 5
  max_steps: 20000
  pretrain_max_steps: 200
  pretrain_max_texts: 100000
  treebank: "UD_Finnish-TDT"
  corpus_ner: "turku-one"
  train_name: "fi_tdt-ud-train"
  dev_name: "fi_tdt-ud-dev"
  test_name: "fi_tdt-ud-test"
  n_threads: 16
  gpu_id: -1
  floret_epochs: 20
  floret_min_count: 100

directories: ["assets", "corpus", "data", "metrics", "training"]

assets:
  - dest: "assets/${vars.treebank}"
    git:
      repo: "https://github.com/UniversalDependencies/${vars.treebank}"
      branch: "r2.9"
      path: ""
  - dest: "assets/${vars.corpus_ner}"
    git:
      repo: "https://github.com/TurkuNLP/turku-one.git"
      branch: "main"
      path: ""
  - dest: "assets/word_frequencies/finnish_vocab.txt.gz"
    url: "http://bionlp-www.utu.fi/.jmnybl/finnish_vocab.txt.gz"

workflows:
  floret-vectors:
    - download-oscar
    - tokenize-oscar
    - train-floret-oscar
  pretrain:
    - init-floret-vectors
    - convert-oscar-jsonl
    - pretrain-oscar
  train-pipeline:
    - init-lexdata
    - init-floret-vectors
    - convert
    - convert-ner
    - train
    - train-ner
    - merge-parser-and-ner
    - evaluate

commands:
  - name: "download-oscar"
    help: "Download a subset of the OSCAR corpus"
    script:
      - "mkdir -p corpus/oscar/"
      - "python -m tools.download_oscar unshuffled_deduplicated_fi ${vars.max_texts} corpus/oscar/unshuffled_deduplicated_fi.txt"
    outputs:
      - "corpus/oscar/unshuffled_deduplicated_fi.txt"

  - name: "tokenize-oscar"
    help: "Tokenize the OSCAR corpus"
    script:
      - "python -m tools.tokenize_fi corpus/oscar/unshuffled_deduplicated_fi.txt corpus/oscar/unshuffled_deduplicated_fi_tokenized.txt"
    deps:
      - "corpus/oscar/unshuffled_deduplicated_fi.txt"
    outputs:
      - "corpus/oscar/unshuffled_deduplicated_fi_tokenized.txt"

  - name: "train-floret-oscar"
    help: "Train floret vectors on the OSCAR corpus"
    script:
      - "floret skipgram -mode floret -hashCount 2 -bucket ${vars.vector_size} -minn ${vars.minn} -maxn ${vars.maxn} -minCount ${vars.floret_min_count} -dim ${vars.vector_dim} -epoch ${vars.floret_epochs} -thread ${vars.n_threads} -input corpus/oscar/unshuffled_deduplicated_fi_tokenized.txt -output vectors/fi-${vars.vector_dim}-${vars.vector_size}-minn${vars.minn}-maxn${vars.maxn}"
      - "gzip -f vectors/fi-${vars.vector_dim}-${vars.vector_size}-minn${vars.minn}-maxn${vars.maxn}.floret"
    deps:
      - "corpus/oscar/unshuffled_deduplicated_fi_tokenized.txt"
    outputs:
      - "vectors/fi-${vars.vector_dim}-${vars.vector_size}-minn${vars.minn}-maxn${vars.maxn}.floret.gz"

  - name: "convert-oscar-jsonl"
    help: "Convert the OSCAR corpus to JSONL format"
    script:
      - "python -m tools.raw_text_to_jsonl --limit ${vars.pretrain_max_texts} corpus/oscar/unshuffled_deduplicated_fi.txt corpus/oscar/unshuffled_deduplicated_fi_${vars.pretrain_max_texts}.jsonl"
    deps:
      - "corpus/oscar/unshuffled_deduplicated_fi.txt"
    outputs:
      - "corpus/oscar/unshuffled_deduplicated_fi_${vars.pretrain_max_texts}.jsonl"

  - name: "pretrain-oscar"
    help: "Pretrain the tok2vec component"
    script:
      - "rm -rf training/pretrain"
      - "mkdir -p training/pretrain"
      - "python -m spacy pretrain configs/fi.cfg training/pretrain --code fi/fi.py --paths.pretrain corpus/oscar/unshuffled_deduplicated_fi_${vars.pretrain_max_texts}.jsonl --paths.vectors data/vectors/fi-${vars.vector_dim}-${vars.vector_size}-minn${vars.minn}-maxn${vars.maxn}-floret --pretraining.max_epochs ${vars.pretrain_max_steps} --gpu-id ${vars.gpu_id}"
      - "cp training/pretrain/models${vars.pretrain_max_steps}.bin pretrain/weights_oscar.bin"
    deps:
      - "configs/fi.cfg"
      - "corpus/oscar/unshuffled_deduplicated_fi_${vars.pretrain_max_texts}.jsonl"
      - "data/vectors/fi-${vars.vector_dim}-${vars.vector_size}-minn${vars.minn}-maxn${vars.maxn}-floret"
    outputs:
      - "pretrain/weights.bin"

  - name: "init-lexdata"
    script:
      - "mkdir -p data/word_frequencies data/vocab"
      - "python -m tools.most_frequent_tokens --num-tokens 500000 assets/word_frequencies/finnish_vocab.txt.gz data/word_frequencies/finnish_vocab_500k.txt.gz"
      - "python -m tools.create_lexdata assets/word_frequencies/finnish_vocab.txt.gz data/word_frequencies/finnish_vocab_500k.txt.gz data/vocab/vocab-data.jsonl"
    deps:
      - "assets/word_frequencies/finnish_vocab.txt.gz"
    outputs:
      - "data/vocab/vocab-data.jsonl"

  - name: "init-floret-vectors"
    help: "Create floret embeddings"
    script:
      - "python -m spacy init vectors fi vectors/fi-${vars.vector_dim}-${vars.vector_size}-minn${vars.minn}-maxn${vars.maxn}.floret.gz data/vectors/fi-${vars.vector_dim}-${vars.vector_size}-minn${vars.minn}-maxn${vars.maxn}-floret --mode floret --name fi_web_floret.vectors"
    deps:
      - "vectors/fi-${vars.vector_dim}-${vars.vector_size}-minn${vars.minn}-maxn${vars.maxn}.floret.gz"
    outputs:
      - "data/vectors/fi-${vars.vector_dim}-${vars.vector_size}-minn${vars.minn}-maxn${vars.maxn}-floret"

  - name: "convert"
    help: "Convert the data to spaCy's format"
    script:
      - "mkdir -p corpus/${vars.treebank}/preprocessed corpus/${vars.treebank}/spacy"
      - "python tools/preprocess_UD-TDT.py assets/${vars.treebank}/${vars.train_name}.conllu corpus/${vars.treebank}/preprocessed/${vars.train_name}.conllu"
      - "python -m spacy convert corpus/${vars.treebank}/preprocessed/${vars.train_name}.conllu corpus/${vars.treebank}/spacy --n-sents 6"
      - "mv corpus/${vars.treebank}/spacy/${vars.train_name}.spacy corpus/${vars.treebank}/spacy/train.spacy"
      - "python tools/preprocess_UD-TDT.py assets/${vars.treebank}/${vars.dev_name}.conllu corpus/${vars.treebank}/preprocessed/${vars.dev_name}.conllu"
      - "python -m spacy convert corpus/${vars.treebank}/preprocessed/${vars.dev_name}.conllu corpus/${vars.treebank}/spacy --n-sents 6"
      - "mv corpus/${vars.treebank}/spacy/${vars.dev_name}.spacy corpus/${vars.treebank}/spacy/dev.spacy"
      - "python tools/preprocess_UD-TDT.py assets/${vars.treebank}/${vars.test_name}.conllu corpus/${vars.treebank}/preprocessed/${vars.test_name}.conllu"
      - "python -m spacy convert corpus/${vars.treebank}/preprocessed/${vars.test_name}.conllu corpus/${vars.treebank}/spacy --n-sents 6"
      - "mv corpus/${vars.treebank}/spacy/${vars.test_name}.spacy corpus/${vars.treebank}/spacy/test.spacy"
    deps:
      - "assets/${vars.treebank}/"
    outputs:
      - "corpus/${vars.treebank}/spacy/train.spacy"
      - "corpus/${vars.treebank}/spacy/dev.spacy"
      - "corpus/${vars.treebank}/spacy/test.spacy"

  - name: "train"
    help: "Train the model"
    script:
      - "python -m spacy train configs/fi.cfg --output training/${vars.treebank}/ --paths.train corpus/${vars.treebank}/spacy/train.spacy --paths.dev corpus/${vars.treebank}/spacy/dev.spacy --paths.init_tok2vec pretrain/weights.bin --paths.vectors data/vectors/fi-${vars.vector_dim}-${vars.vector_size}-minn${vars.minn}-maxn${vars.maxn}-floret --code fi/fi.py --gpu-id ${vars.gpu_id} --training.max_steps ${vars.max_steps}"
    deps:
      - "configs/fi.cfg"
      - "corpus/${vars.treebank}/spacy/train.spacy"
      - "corpus/${vars.treebank}/spacy/dev.spacy"
      - "pretrain/weights.bin"
      - "data/vocab/vocab-data.jsonl"
      - "data/vectors/fi-${vars.vector_dim}-${vars.vector_size}-minn${vars.minn}-maxn${vars.maxn}-floret"
    outputs:
      - "training/${vars.treebank}/model-best"

  - name: "convert-ner"
    help: "Convert the NER corpus to spaCy's format"
    script:
      - "mkdir -p corpus/${vars.corpus_ner}/spacy"
      - "python -m spacy convert assets/${vars.corpus_ner}/data/conll/train.tsv corpus/${vars.corpus_ner}/spacy --converter ner --n-sents 10"
      - "python -m spacy convert assets/${vars.corpus_ner}/data/conll/dev.tsv corpus/${vars.corpus_ner}/spacy --converter ner --n-sents 10"
      - "python -m spacy convert assets/${vars.corpus_ner}/data/conll/test.tsv corpus/${vars.corpus_ner}/spacy --converter ner --n-sents 10"
    deps:
      - "assets/${vars.corpus_ner}/"
    outputs:
      - "corpus/${vars.corpus_ner}/spacy/train.spacy"
      - "corpus/${vars.corpus_ner}/spacy/dev.spacy"
      - "corpus/${vars.corpus_ner}/spacy/test.spacy"

  - name: "train-ner"
    help: "Train the NER model"
    script:
      - "python -m spacy train configs/fi-ner.cfg --output training/${vars.corpus_ner}/ --paths.train corpus/${vars.corpus_ner}/spacy/train.spacy --paths.dev corpus/${vars.corpus_ner}/spacy/dev.spacy --paths.init_tok2vec pretrain/weights.bin --paths.vectors data/vectors/fi-${vars.vector_dim}-${vars.vector_size}-minn${vars.minn}-maxn${vars.maxn}-floret --code fi/fi.py --gpu-id ${vars.gpu_id} --training.max_steps ${vars.max_steps}"
    deps:
      - "configs/fi-ner.cfg"
      - "corpus/${vars.corpus_ner}/spacy/train.spacy"
      - "corpus/${vars.corpus_ner}/spacy/dev.spacy"
      - "pretrain/weights.bin"
      - "data/vectors/fi-${vars.vector_dim}-${vars.vector_size}-minn${vars.minn}-maxn${vars.maxn}-floret"
    outputs:
      - "training/${vars.corpus_ner}/model-best"

  - name: "merge-parser-and-ner"
    help: "Merge the parser and NER models into one model"
    script:
      - "python -m tools.mergemodels training/${vars.treebank}/model-best training/${vars.corpus_ner}/model-best training/merged"
    deps:
      - "training/${vars.treebank}/model-best"
      - "training/${vars.corpus_ner}/model-best"
    outputs:
      - "training/merged"

  - name: "evaluate"
    help: "Evaluate the NER model"
    script:
      - "mkdir -p metrics/${vars.treebank}"
      - "mkdir -p metrics/${vars.corpus_ner}"
      - "python -m spacy evaluate training/merged corpus/${vars.treebank}/spacy/dev.spacy --output metrics/${vars.treebank}/dev.json --code fi/fi.py --gpu-id ${vars.gpu_id}"
      - "python -m spacy evaluate training/merged corpus/${vars.treebank}/spacy/test.spacy --output metrics/${vars.treebank}/test.json --code fi/fi.py --gpu-id ${vars.gpu_id}"
      - "python -m spacy evaluate training/merged corpus/${vars.corpus_ner}/spacy/dev.spacy --output metrics/${vars.corpus_ner}/dev.json --code fi/fi.py --gpu-id ${vars.gpu_id}"
      - "python -m spacy evaluate training/merged corpus/${vars.corpus_ner}/spacy/test.spacy --output metrics/${vars.corpus_ner}/test.json --code fi/fi.py --gpu-id ${vars.gpu_id}"
    deps:
      - "corpus/${vars.treebank}/spacy/dev.spacy"
      - "corpus/${vars.treebank}/spacy/test.spacy"
      - "corpus/${vars.corpus_ner}/spacy/dev.spacy"
      - "corpus/${vars.corpus_ner}/spacy/test.spacy"
      - "training/merged"
    outputs:
      - "metrics/${vars.corpus_ner}/dev.json"
      - "metrics/${vars.corpus_ner}/test.json"
